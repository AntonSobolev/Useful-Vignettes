},error=function(e){cat("ERROR :",conditionMessage(e), "\n")}) # End of try catch
if(nchar(ogrn[[1]])!=0){
d.new.line <- cbind.fill(company,region,zip,address,ogrn,inn,
fill=NA) %>% data.table() # fill empty elements of scraped data
d.new.line <- cbind(company_real,region_real,d.new.line) # add real company and region to data
names(d.new.line) <- c("company_real","region_real","company","region","zip","address","ogrn","inn")
write.table(d.new.line, "218-scrape-results.csv", append = T, quote=F, row.names = FALSE, col.names = F, sep = "\t")
print(d.new.line$zip[1])
print("File is written")
}
remDr$close()
}
p_load(data.table, dplyr, tidyr, stringr, methods, rowr) # Packages for text mining
# Cleaning Functions
clean_up <- function (x){
x <- gsub("[[:punct:]]"," ", x)
x <- gsub("\r", "", x)
x <- gsub("\n", "", x)
x <- gsub("\\s+", " ", x)
x <- gsub("^\\s+", "", x)
x <- gsub("\\s+$", "", x)
# x <- gsub("[ ]", "+", x)
return(x)
}
clean_up2 <- function (x){
x <- gsub(";"," ", x)
return(x)
}
i = 35
for (i in 1:nrow(ngo)){
Sys.sleep(3)
company_real <- ngo$company_real[i]
region_real <- ngo$region_real[i]
remDr <- remoteDriver(remoteServerAddr = "localhost"
, port = 4445L
, browserName = "firefox"
)
company_real_for_url <-gsub(" ", "+", company_real)
company_real_for_url <-gsub("\\s+$", "", company_real_for_url)
region_real_for_url <-gsub(" ", "+", region_real)
region_real_for_url <-gsub("\\s+$", "", region_real_for_url)
url <- paste0('https://focus.kontur.ru/search?region=&industry=&state=081077917&query='
,company_real_for_url, "+", region_real_for_url)
remDr$open(silent = TRUE)
remDr$navigate(url)
ogrn <- ""
tryCatch({
# Extract ogrn and date
ogrn_large <- remDr$findElements(using = 'css selector', ".ind1em+ .ind1em")
ogrn_large <- unlist(lapply(ogrn_large, function(x){x$getElementText()}))
# Extract OGRN
ogrn_pattern <- "ОГРН: (.*?) – "
ogrn <- regmatches(ogrn_large,regexec(ogrn_pattern,ogrn_large))
ogrn <- sapply(ogrn, "[[", 2)
# Extract DATE
date_pattern <- " . (.*)"
date <- regmatches(ogrn_large,regexec(date_pattern,ogrn_large))
date <- sapply(date, "[[", 2)
# Extract NAME
company <- remDr$findElements(using = 'css selector', ".js-subject-link")
company <- unlist(lapply(company, function(x){x$getElementText()})) %>% clean_up()
# Extract INN
inn <- remDr$findElements(using = 'css selector', ".ind1em:nth-child(1)")
inn <- unlist(lapply(inn, function(x){x$getElementText()}))
inn_pattern <- "ИНН: (.*)"
inn <- regmatches(inn,regexec(inn_pattern,inn))
inn <- lapply(inn, `length<-`, max(lengths(inn)))
inn <- sapply(inn, "[[", 2)
# Extract ADDRESS & REGION & ZIP
address <- remDr$findElements(using = 'css selector', ".noMargin.darkgreen")
address <- unlist(lapply(address, function(x){x$getElementText()}))
address <- clean_up2(address) # get rid of ";" in your address string
address.update <- strsplit(address, ",")
# Extract REGION
region <- sapply(address.update, "[[", 2) %>% clean_up()
# Extract ZIP
zip <- sapply(address.update, "[[", 1) %>% clean_up()
# End of try catch
print(paste(i, zip[[1]]))
},error=function(e){cat("ERROR :",conditionMessage(e), "\n")}) # End of try catch
if(nchar(ogrn[[1]])!=0){
d.new.line <- cbind.fill(company,region,zip,address,ogrn,inn,
fill=NA) %>% data.table() # fill empty elements of scraped data
d.new.line <- cbind(company_real,region_real,d.new.line) # add real company and region to data
names(d.new.line) <- c("company_real","region_real","company","region","zip","address","ogrn","inn")
write.table(d.new.line, "218-scrape-results.csv", append = T, quote=F, row.names = FALSE, col.names = F, sep = "\t")
print(d.new.line$zip[1])
print("File is written")
}
remDr$close()
}
zip
class(zip)
zip[[1]]
if (!require("pacman")) install.packages("pacman")
p_load(data.table, dplyr, haven, ggplot2,ggmap, MASS)
## generate some random data
data <- data.frame(a=abs(rnorm(1000, mean=0.5)), b=abs(rnorm(1000, mean=0.5)))
data$a <- data$a / max(data$a)
data$b <- data$b / max(data$b)
## layout settings for ggplot
t2 <- theme(
axis.line = element_line(colour = "black"),
axis.text = element_text(colour = "black"),
axis.ticks = element_line(colour = "black"),
panel.grid.major.x = element_blank(),
panel.grid.minor.x = element_blank(),
panel.grid.major.y = element_blank(),
panel.grid.minor.y = element_blank(),
panel.border = element_blank(),
panel.background = element_blank()
)
## generate the "z" coordinate (density) just for the correct midpoint in the color gradient
z <- kde2d(data$a, data$b)
g <- ggplot(data, aes(x=a, y=b)) +
stat_density2d(aes(fill=..density..), geom="tile", contour=FALSE) +
scale_fill_gradient2(low="#44aa00", mid="#ffcc00", high="#502d16", midpoint=mean(range(z$z))) +
## limit scale ( density is calculated on the limited scale, all other points are removed from calculation)
#xlim(0,1) +
#ylim(0,1) +
## limit view area ( density is calcluated on all points, no points removed )
#coord_cartesian(xlim = c(0, 1), ylim=c(0,1)) +
xlab("x method") +
ylab("y method") +
## add points to the density map (comment it if not desired)
geom_point(size=0.6, colour="white",shape=1, alpha=0.2) +
## make a line from (0,0) - (1,1)
#geom_segment(aes(x=0, y=0, xend=1, yend=1)) +
## or just create a abline with slope 1 and intercept 0
t2
pdf("density.pdf", width=0.25*6+6, height=6)
g
d <- fread("WM_March_merged_data_geo_data_only.csv", stringsAsFactors = FALSE)
d <- d[,c("ID","utc_timestamp","latitude", "longitude"), with = F]
d$utc_timestamp <- d$utc_timestamp - min(d$utc_timestamp)
d[,hour:=utc_timestamp / (60*60) ][,hour:=round(hour, digits = 0),] # calculate hours assuming 3600 secs in one hour
d.by.hour <- d[,.(longitude = mean(longitude),
latitude = mean(latitude)),
by = c("ID", "hour")
]
z <- kde2d(d$latitude, d$longitude)
g <- ggplot(d.by.hour, aes(x = longitude,y= latitude)) +
stat_density2d(aes(fill=..density..), geom="tile", contour=FALSE) +
scale_fill_gradient2(low="#44aa00", mid="#ffcc00", high="#502d16", midpoint=mean(range(z$z))) +
## limit scale ( density is calculated on the limited scale, all other points are removed from calculation)
#xlim(0,1) +
#ylim(0,1) +
## limit view area ( density is calcluated on all points, no points removed )
#coord_cartesian(xlim = c(0, 1), ylim=c(0,1)) +
xlab("x method") +
ylab("y method") +
## add points to the density map (comment it if not desired)
geom_point(size=0.6, colour="white",shape=1, alpha=0.2) +
## make a line from (0,0) - (1,1)
#geom_segment(aes(x=0, y=0, xend=1, yend=1)) +
## or just create a abline with slope 1 and intercept 0
t2
pdf("density.pdf", width=0.25*6+6, height=6)
g <- ggplot(d.by.hour, aes(x = longitude,y= latitude)) +
stat_density2d(aes(fill=..density..), geom="tile", contour=FALSE) +
scale_fill_gradient2(low="#44aa00", mid="#ffcc00", high="#502d16", midpoint=mean(range(z$z))) +
## limit scale ( density is calculated on the limited scale, all other points are removed from calculation)
#xlim(0,1) +
#ylim(0,1) +
## limit view area ( density is calcluated on all points, no points removed )
#coord_cartesian(xlim = c(0, 1), ylim=c(0,1)) +
xlab("x method") +
ylab("y method") +
## add points to the density map (comment it if not desired)
geom_point(size=0.6, colour="white",shape=1, alpha=0.2) +
## make a line from (0,0) - (1,1)
#geom_segment(aes(x=0, y=0, xend=1, yend=1)) +
## or just create a abline with slope 1 and intercept 0
t2
pdf(g,"density.pdf", width=0.25*6+6, height=6)
pdf(g,"density.pdf", width=0.25*6+6, height=6)
g
if (!require("pacman")) install.packages("pacman")
p_load(data.table, dplyr, haven, ggplot2,ggmap, MASS)
p_load(data.table, dplyr, haven, ggplot2,tidyr, tibble, ggmap, gganimate, magick)
g
g <- ggplot(d.by.hour, aes(x = longitude,y= latitude)) +
stat_density2d(aes(fill=..density..), geom="tile", contour=FALSE) +
scale_fill_gradient2(low="#44aa00", mid="#ffcc00", high="#502d16", midpoint=mean(range(z$z))) +
## limit scale ( density is calculated on the limited scale, all other points are removed from calculation)
xlim(77.02,77.04) +
ylim(38.90,38.92) +
## limit view area ( density is calcluated on all points, no points removed )
#coord_cartesian(xlim = c(0, 1), ylim=c(0,1)) +
xlab("x method") +
ylab("y method") +
## add points to the density map (comment it if not desired)
geom_point(size=0.6, colour="white",shape=1, alpha=0.2) +
## make a line from (0,0) - (1,1)
#geom_segment(aes(x=0, y=0, xend=1, yend=1)) +
## or just create a abline with slope 1 and intercept 0
t2
g
if (!require("pacman")) install.packages("pacman")
p_load(data.table, dplyr, methods, keras)
#install_keras()
d <- fread("5-imputed-linear-long-data-for-deep-learning.csv", stringsAsFactors = FALSE)
protest.IDs <- fread("3-core-protesters-ID-manual.csv", stringsAsFactors = FALSE)
non.protest.IDs <- fread("3-definite-non-protesters-ID.csv", stringsAsFactors = FALSE)
d.trajectory <- d[,-1]
d.trajectory <- d.trajectory + 100
d.trajectory$longitude.14 <- d.trajectory$longitude.14 - d.trajectory$longitude.13
d.trajectory$longitude.15 <- d.trajectory$longitude.15 - d.trajectory$longitude.13
d.trajectory$longitude.16 <- d.trajectory$longitude.16 - d.trajectory$longitude.13
d.trajectory$longitude.17 <- d.trajectory$longitude.17 - d.trajectory$longitude.13
d.trajectory$longitude.18 <- d.trajectory$longitude.18 - d.trajectory$longitude.13
d.trajectory$longitude.19 <- d.trajectory$longitude.19 - d.trajectory$longitude.13
d.trajectory$longitude.13 <- d.trajectory$longitude.13 - d.trajectory$longitude.13
d.trajectory$latitude.14 <- d.trajectory$latitude.14 - d.trajectory$latitude.13
d.trajectory$latitude.15 <- d.trajectory$latitude.15 - d.trajectory$latitude.13
d.trajectory$latitude.16 <- d.trajectory$latitude.16 - d.trajectory$latitude.13
d.trajectory$latitude.17 <- d.trajectory$latitude.17 - d.trajectory$latitude.13
d.trajectory$latitude.18 <- d.trajectory$latitude.18 - d.trajectory$latitude.13
d.trajectory$latitude.19 <- d.trajectory$latitude.19 - d.trajectory$latitude.13
d.trajectory$latitude.13 <- d.trajectory$latitude.13 - d.trajectory$latitude.13
d.trajectory <- d.trajectory[,-c(1:2)]
d$group <- 'unknown'
d$group[d$ID %in% protest.IDs$ID] <- 'definite protesters'
d$group[d$ID %in% non.protest.IDs$ID] <- 'definte non-protesters'
d.trajectory.with.labels <- cbind(label = d$group, d.trajectory) %>% data.table()
d.trajectory.with.labels <- d.trajectory.with.labels[d.trajectory.with.labels$label!='unknown',]
y <- ifelse(d.trajectory.with.labels$label=="definite protesters", 1,0)
set.seed(1234)
nrow(d.trajectory.with.labels) / 3
test.index <- sample(nrow(d.trajectory.with.labels),nrow(d.trajectory.with.labels) / 3,  replace = F)
d.trajectory.with.labels[,-1] <- scale(d.trajectory.with.labels[,-1])
d.trajectory.with.labels[,-1]
if (!require("pacman")) install.packages("pacman")
p_load(data.table, dplyr, methods, keras)
#install_keras()
d <- fread("5-imputed-linear-long-data-for-deep-learning.csv", stringsAsFactors = FALSE)
protest.IDs <- fread("3-core-protesters-ID-manual.csv", stringsAsFactors = FALSE)
non.protest.IDs <- fread("3-definite-non-protesters-ID.csv", stringsAsFactors = FALSE)
d.trajectory <- d[,-1]
d.trajectory <- d.trajectory + 100
d.trajectory$longitude.14 <- d.trajectory$longitude.14 - d.trajectory$longitude.13
d.trajectory$longitude.15 <- d.trajectory$longitude.15 - d.trajectory$longitude.13
d.trajectory$longitude.16 <- d.trajectory$longitude.16 - d.trajectory$longitude.13
d.trajectory$longitude.17 <- d.trajectory$longitude.17 - d.trajectory$longitude.13
d.trajectory$longitude.18 <- d.trajectory$longitude.18 - d.trajectory$longitude.13
d.trajectory$longitude.19 <- d.trajectory$longitude.19 - d.trajectory$longitude.13
d.trajectory$longitude.13 <- d.trajectory$longitude.13 - d.trajectory$longitude.13
d.trajectory$latitude.14 <- d.trajectory$latitude.14 - d.trajectory$latitude.13
d.trajectory$latitude.15 <- d.trajectory$latitude.15 - d.trajectory$latitude.13
d.trajectory$latitude.16 <- d.trajectory$latitude.16 - d.trajectory$latitude.13
d.trajectory$latitude.17 <- d.trajectory$latitude.17 - d.trajectory$latitude.13
d.trajectory$latitude.18 <- d.trajectory$latitude.18 - d.trajectory$latitude.13
d.trajectory$latitude.19 <- d.trajectory$latitude.19 - d.trajectory$latitude.13
d.trajectory$latitude.13 <- d.trajectory$latitude.13 - d.trajectory$latitude.13
d.trajectory <- d.trajectory[,-c(1:2)]
d.trajectory <- scale(d.trajectory)
d$group <- 'unknown'
d$group[d$ID %in% protest.IDs$ID] <- 'definite protesters'
d$group[d$ID %in% non.protest.IDs$ID] <- 'definte non-protesters'
d.trajectory.with.labels <- cbind(label = d$group, d.trajectory) %>% data.table()
d.trajectory.with.labels <- d.trajectory.with.labels[d.trajectory.with.labels$label!='unknown',]
y <- ifelse(d.trajectory.with.labels$label=="definite protesters", 1,0)
set.seed(1234)
nrow(d.trajectory.with.labels) / 3
test.index <- sample(nrow(d.trajectory.with.labels),nrow(d.trajectory.with.labels) / 3,  replace = F)
x_test <- d.trajectory.with.labels[test.index,-1] %>% as.matrix(x_test)
x_train <- d.trajectory.with.labels[-test.index,-1] %>% as.matrix()
y_test <- y[test.index] %>% as.matrix()
y_train <- y[-test.index] %>% as.matrix()
set.seed(1234)
test.sample.size <- 1879
# test.index <- sample(nrow(d.trajectory.with.labels),nrow(d.trajectory.with.labels) / 3,  replace = F)
test.index <- sample(nrow(d.trajectory.with.labels),1879,  replace = F)
x_test <- d.trajectory.with.labels[test.index,-1] %>% as.matrix(x_test)
x_train <- d.trajectory.with.labels[-test.index,-1] %>% as.matrix()
y_test <- y[test.index] %>% as.matrix()
y_train <- y[-test.index] %>% as.matrix()
batch_size <- 2000
num_classes <- 2
epochs <- 100
# Convert class vectors to binary class matrices
y_train <- to_categorical(y_train, num_classes)
y_test <- to_categorical(y_test, num_classes)
model <- keras_model_sequential()
model %>%
layer_dense(units = 100, activation = 'relu', input_shape = c(12)) %>%
layer_dropout(rate = 0.3) %>%
layer_dense(units = 100, activation = 'relu') %>%
layer_dropout(rate = 0.3) %>%
layer_dense(units = 100, activation = 'relu') %>%
layer_dropout(rate = 0.3) %>%
layer_dense(units = 100, activation = 'relu') %>%
layer_dropout(rate = 0.3) %>%
layer_dense(units = 100, activation = 'relu') %>%
layer_dropout(rate = 0.3) %>%
layer_dense(units = 2, activation = 'softmax')
summary(model)
model %>% compile(
loss = 'categorical_crossentropy',
optimizer = optimizer_rmsprop(),
metrics = c('accuracy')
)
history <- model %>% fit(
x_train, y_train,
batch_size = batch_size,
epochs = epochs,
verbose = 1,
validation_split = 0.3
)
batch_size <- 2000
num_classes <- 2
epochs <- 1000
model <- keras_model_sequential()
model %>%
layer_dense(units = 100, activation = 'relu', input_shape = c(12)) %>%
layer_dropout(rate = 0.3) %>%
layer_dense(units = 100, activation = 'relu') %>%
layer_dropout(rate = 0.3) %>%
layer_dense(units = 100, activation = 'relu') %>%
layer_dropout(rate = 0.3) %>%
layer_dense(units = 100, activation = 'relu') %>%
layer_dropout(rate = 0.3) %>%
layer_dense(units = 100, activation = 'relu') %>%
layer_dropout(rate = 0.3) %>%
layer_dense(units = 2, activation = 'softmax')
summary(model)
model %>% compile(
loss = 'categorical_crossentropy',
optimizer = optimizer_rmsprop(),
metrics = c('accuracy')
)
history <- model %>% fit(
x_train, y_train,
batch_size = batch_size,
epochs = epochs,
verbose = 1,
validation_split = 0.3
)
batch_size <- 3000
num_classes <- 2
epochs <- 1000
history <- model %>% fit(
x_train, y_train,
batch_size = batch_size,
epochs = epochs,
verbose = 1,
validation_split = 0.3
)
batch_size <- 2000
num_classes <- 2
epochs <- 1000
model <- keras_model_sequential()
model %>%
layer_dense(units = 100, activation = 'relu', input_shape = c(12)) %>%
layer_dropout(rate = 0.3) %>%
layer_dense(units = 50, activation = 'relu') %>%
layer_dropout(rate = 0.3) %>%
layer_dense(units = 50, activation = 'relu') %>%
layer_dropout(rate = 0.3) %>%
layer_dense(units = 50, activation = 'relu') %>%
layer_dropout(rate = 0.3) %>%
layer_dense(units = 50, activation = 'relu') %>%
layer_dropout(rate = 0.3) %>%
layer_dense(units = 2, activation = 'softmax')
summary(model)
batch_size <- 2000
num_classes <- 2
epochs <- 300
history <- model %>% fit(
x_train, y_train,
batch_size = batch_size,
epochs = epochs,
verbose = 1,
validation_split = 0.3,
callbacks = callback_tensorboard("logs/run_a")
)
model <- keras_model_sequential()
model %>%
layer_dense(units = 100, activation = 'relu', input_shape = c(12)) %>%
layer_dropout(rate = 0.3) %>%
layer_dense(units = 50, activation = 'relu') %>%
layer_dropout(rate = 0.3) %>%
layer_dense(units = 50, activation = 'relu') %>%
layer_dropout(rate = 0.3) %>%
layer_dense(units = 50, activation = 'relu') %>%
layer_dropout(rate = 0.3) %>%
layer_dense(units = 50, activation = 'relu') %>%
layer_dropout(rate = 0.3) %>%
layer_dense(units = 2, activation = 'softmax')
summary(model)
model %>% compile(
loss = 'categorical_crossentropy',
optimizer = optimizer_rmsprop(),
metrics = c('accuracy')
)
history <- model %>% fit(
x_train, y_train,
batch_size = batch_size,
epochs = epochs,
verbose = 1,
validation_split = 0.3,
callbacks = callback_tensorboard("logs/run_a")
)
tensorboard("logs/run_a")
# Fit model to data
history <- model %>% fit(
x_train, y_train,
batch_size = batch_size,
epochs = epochs,
verbose = 1,
validation_split = 0.3,
callbacks = callback_tensorboard("logs/run_a")
)
# run brew in terminal to install imagemagick:
# brew install imagemagick
if (!require("pacman")) install.packages("pacman")
if (!require("gganimate")) devtools::install_github("dgrtwo/gganimate")
p_load(data.table, dplyr, haven, ggplot2,tidyr, tibble, ggmap, gganimate, magick,geohash, animation)
p_load(imputeTS)
d <- fread("WM_March_merged_data_geo_data_only.csv", stringsAsFactors = FALSE)
d <- d[,c("ID","utc_timestamp","latitude", "longitude"), with = F]
# Addjust to the first observation at t = 0
d$utc_timestamp <- d$utc_timestamp - min(d$utc_timestamp)
d[,hour:=utc_timestamp / (60*60) ][,hour:=round(hour, digits = 0),] # calculate hours
# Reduce to hour level
d.by.hour <- d[,.(longitude = mean(longitude),
latitude = mean(latitude)),
by = c("ID", "hour")
]
# Keep only hours 13 to 19
d.by.hour  <- d.by.hour[hour >=10 & hour<=20,] # reduce to 13:19 after imputations
d.by.hour$num <- 1
d.by.hour[,observations_for_ID:=sum(num), by=c("ID")]
# Keep observations with at least three time-stamps during this day
d.by.hour <- d.by.hour[d.by.hour$observations_for_ID>=3,,]
d.by.hour[,num:=NULL]
d.by.hour[,observations_for_ID:=NULL]
# Expand grid to such that all ID had the same number of observations
expanded.grid <- expand.grid(hour = c(10:20), ID = unique(d.by.hour$ID))
# merge the grid to original data
d.by.hour <- merge(expanded.grid, d.by.hour, by = c("ID","hour"), all = TRUE) %>% data.table()
# Long to wide data
d.by.hour.wide <- d.by.hour[hour >=13 & hour<=19,]
d.by.hour.wide <- reshape(d.by.hour.wide, idvar = "ID", timevar = "hour", direction = "wide") %>% data.table()
p_load(VIM) # Package to visualize missings
aggr_plot <- aggr(d.by.hour.wide, col=c('navyblue','red'), numbers=TRUE, sortVars=FALSE, labels=names(d.by.hour.wide), cex.axis=.7, gap=3, ylab=c("Histogram of missing data","Pattern"))
#install.packages('pacman')
require("knitr")
require(pacman)
p_load(data.table, dplyr, plyr, tidyr, scales, XML, lubridate, mvmeta) # Packages for text mining
p_load(ggplot2,ggbiplot) # Packages for graphics
# library(devtools)
# install_github("ggbiplot", "vqv")
# Table of estimated topics of initial posts
t <- fread('true_vs_predicted_sample.csv', sep = ",", header= TRUE, stringsAsFactors = F)
names(t) <- c('Type', 'LogRegression', 'NB_gaus', 'NB_multinomial', 'RandomForest', 'RandomForest_xgb', 'SVM_linear', 'SWM_rbf')
t.tidy <- t %>% gather(model, score, -Type) %>% as.data.table()
t.tidy$Type2 <- 'Random User'
t.tidy$Type2[t.tidy$Type==1] <- 'Troll'
ggplot(t.tidy[t.tidy$model == 'LogRegression' | t.tidy$model == 'RandomForest'], aes(x = score, fill = as.factor(Type2))) +
geom_density(alpha = .9) +
facet_wrap( ~ model) +
theme_bw() + theme(legend.position='bottom', legend.title = element_blank()) + scale_colour_grey() + ggtitle('Out-of-Sample Predictions')
# Table of estimated topics of initial posts
t.predicted <- fread('true_vs_predicted_new.csv', sep = ",", header= TRUE, stringsAsFactors = F)
names(t.predicted) <- c('Type', 'LogRegression', 'NB_gaus', 'NB_multinomial', 'RandomForest', 'RandomForest_xgb', 'SVM_linear', 'SWM_rbf')
t.predicted.tidy <- t.predicted %>% gather(model, score, -Type) %>% as.data.table()
t.predicted.tidy$Type2 <- 'Test-User'
t.list <- list(t.predicted.tidy, t.tidy)
t <- rbindlist(t.list)
ggplot(t[t$model == 'RandomForest'], aes(x = score, fill = as.factor(Type2))) +
geom_density(alpha = .6) +
facet_wrap( ~ model) +
general.hyp + theme(legend.position='bottom', legend.title = element_blank()) + scale_colour_grey() + ggtitle('Out-of-Sample Predictions')
x.data <- fread('X_data.csv', sep = ",", header= FALSE, stringsAsFactors = F)
y.data <- fread('Y_data.csv', sep = ",", header= FALSE, stringsAsFactors = F)
CP <- prcomp(x.data[,c(c(1:50),c(80:100))], scale. = T, rank. = 2)
n
x.data$type <- 'Troll'
x.data$type[y.data$V2 == 1] <- 'Random User'
ggbiplot(CP, obs.scale = 1, var.scale = 1, groups = as.factor(x.data$type), ellipse = FALSE, circle = TRUE, var.axes = F, alpha = .4, shape = ) + xlim(-2,8) + ylim(-5,5) +
theme_bw() + theme(legend.position='bottom', legend.title = element_blank()) + ggtitle('PCA')
d <- CP$x[,1:2] %>% data.table()
d$type <- x.data$type
names(d)
g <- ggplot(d, aes(y= PC2, x = PC1, col = type, shape = type))+ geom_point(size = 2) + xlim(-2.5,8) + ylim(-7,7) + theme_bw() + text = element_text(size=16)
g <- ggplot(d, aes(y= PC2, x = PC1, col = type, shape = type))+ geom_point(size = 2) + xlim(-2.5,8) + ylim(-7,7) + theme_bw() + text = element_text(size=16)
g <- ggplot(d, aes(y= PC2, x = PC1, col = type, shape = type))+ geom_point(size = 2) + xlim(-2.5,8) + ylim(-7,7) + theme_bw() + theme(text = element_text(size=16))
g2 <- g + theme(legend.position="bottom")
g2 + scale_shape_manual(values = c(3,4))
getwd()
if (!require("pacman")) install.packages("pacman")
p_load(data.table, dplyr, methods, keras, gbm, caret, pROC, devtools)
devtools::install_github("bnosac/cronR")
install.packages('miniUI')
install.packages('shiny')
#install_keras()
p_load(cronR)
cronR:::cron_rstudioaddin()
#install_keras()
p_load(cronR,miniUI,shiny, shinyFiles )
cronR:::cron_rstudioaddin()
getwd()
smth <- "hello world"
write.table(smth, "result.csv")
write.table(smth, "new-results.csv")
setwd("/Users/sobol/Documents/GitHub/Useful-Vignettes/Automated-Scripts-croneR")
write.table(smth, "new-results.csv")
cronR:::cron_rstudioaddin()
cronR:::cron_rstudioaddin()
cronR:::cron_rstudioaddin()
cronR:::cron_rstudioaddin()
